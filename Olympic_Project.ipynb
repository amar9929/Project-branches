{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Olympic Project - road to gold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions:\n",
    " 1. How has the performance of athletes changed based on gender, and has this led to a reduction in the performance gap?\n",
    " 2. Can past Olympic results reliably predict future outcomes?\n",
    "\n",
    "## Hypothesis:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. As women have increasingly engaged in the Olympic Games and gained more equitable chances to train and compete, the performance gap should have diminished over the last century and is expected to continue decreasing in every competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-How has the performance of athletes changed based on gender, and has this led to a reduction in the performance gap?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Olympic performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import of the data frame\n",
    "from selenium import webdriver \n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the imformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_extraction(soup):\n",
    "    ''' this function extract the information of a specifique event from the olympic web page using web scrapping\n",
    "    '''\n",
    "    \n",
    "    #location and indentification of the required information\n",
    "    table = soup.find('div',attrs={'data-cy':'table-content'})\n",
    "    rows = table.find_all('div',attrs={'data-row-id':True})\n",
    "    #list of the info to capture\n",
    "    countries = []\n",
    "    participant=[]\n",
    "    results=[]\n",
    "    #loop to extract all events information of the dicipline selected\n",
    "    for row in rows:\n",
    "        try:\n",
    "            countries.append(row.find('span',attrs={'class':'styles__CountryName-sc-1r5phm6-1 eQULfE'}).text)\n",
    "        except:\n",
    "            countries.append(None)\n",
    "        try:\n",
    "            participant.append(row.find('h3',attrs={'data-cy':'athlete-name'}).text)\n",
    "        except:\n",
    "            participant.append(None)\n",
    "        try:\n",
    "            results.append(row.find('span',attrs={'data-cy':'result-info-content'}).text)\n",
    "        except:\n",
    "            results.append(None)\n",
    "    info_event=pd.DataFrame({'country':countries,'participant':participant,'result':results})\n",
    "    #add a column with the olympics games name\n",
    "    olympicg= soup.find('button',attrs={'data-cy':'game-select'}).text\n",
    "    info_event['olympic_game']=olympicg\n",
    "    #add a column with the dicipline name\n",
    "    discipline=soup.find('button',attrs={'data-cy':'discipline-select'}).text\n",
    "    info_event['discipline']=discipline\n",
    "    #add a column with the event name\n",
    "    event=soup.find('button',attrs={'data-cy':'event-select'}).text\n",
    "    info_event['event']=event\n",
    "    #return the entire dataset with all the required information\n",
    "    return info_event\n",
    "\n",
    "def data_grab(url):\n",
    "    ''' from the url of a specific discipline at the select olympic game, this function upload the pages of all event to be able to extract the information.\n",
    "    It return a csv files in order to use the information later.'''\n",
    "    #create the list with the required information\n",
    "    final_result=[]\n",
    "    #calculate the number of event in the dicipline selected\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "    button_event = driver.find_element('css selector','button[data-cy=event-select]')\n",
    "    button_event.click()\n",
    "    events = len(driver.find_elements('css selector','button[data-cy=event-button]'))\n",
    "    button_event.click()\n",
    "    #loop to go though all the url and grab the required information or print the url where it did't work\n",
    "    try:    \n",
    "        for i in range(events):\n",
    "            button_event = driver.find_element('css selector','button[data-cy=event-select]')\n",
    "            button_event.click()\n",
    "            eventfor=driver.find_elements('css selector','button[data-cy=event-button]')\n",
    "            eventfor=eventfor[i]\n",
    "            eventfor.click()\n",
    "            #Go to the result of the event selected\n",
    "            button_go = driver.find_element('css selector','a[data-cy=go-link]')\n",
    "            button_go.click()\n",
    "            time.sleep(5)\n",
    "            soup= BeautifulSoup(driver.page_source)\n",
    "            final_result.append(info_extraction(soup))\n",
    "        pd.concat(final_result).to_csv(url.split('olympic-games/')[1].replace('/','_')+'.csv')\n",
    "    except:\n",
    "        print(url)\n",
    "        pass\n",
    "\n",
    "    def check_valid_urls(urls)\n",
    "    \"\"\"Removes urls from the list that have a 400 status\n",
    "        Parameters:\n",
    "            urls- a iterable with url in string format\n",
    "        Return:\n",
    "            a list of working urls\n",
    "    \"\"\"\n",
    "    driver = webdriver.Chrome()\n",
    "    valid_urls=[]\n",
    "    for url in urls:\n",
    "        driver.get(url)\n",
    "        if not driver.current_url.endswith('404.html'):\n",
    "            valid_urls.append(url)\n",
    "    return valid_urls.to_csv('olympics_url'+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extracting code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list=pd.read_csv('olympics_url.csv').iloc[:,1].to_list()\n",
    "driver = webdriver.Chrome()\n",
    "driver.get('https://olympics.com/en/olympic-games/beijing-2022/results/alpine-skiing')\n",
    "time.sleep(5)\n",
    "cookies_button = driver.find_element('css selector','#onetrust-accept-btn-handler')\n",
    "cookies_button.click()\n",
    "time.sleep(5)\n",
    "list(map(data_grab,url_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#select the event with comparable result\u001b[39;00m\n\u001b[0;32m      2\u001b[0m csv_list \u001b[38;5;241m=\u001b[39m [pd\u001b[38;5;241m.\u001b[39mread_csv(fil) \u001b[38;5;28;01mfor\u001b[39;00m fil \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir() \u001b[38;5;28;01mif\u001b[39;00m fil\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mswimming.csv\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m fil\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweightlifting.csv\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m fil\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mathletics.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m              \u001b[38;5;129;01mor\u001b[39;00m fil\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcycling-track.csv\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m fil\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrowing.csv\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m fil\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msailing.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m              ]\n\u001b[1;32m----> 5\u001b[0m data_olympic\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mconcat(csv_list)\n\u001b[0;32m      6\u001b[0m data_olympic\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnnamed: 0\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m'\u001b[39m},inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#drop rows if result==nan\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:372\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    370\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 372\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[0;32m    373\u001b[0m     objs,\n\u001b[0;32m    374\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m    375\u001b[0m     ignore_index\u001b[38;5;241m=\u001b[39mignore_index,\n\u001b[0;32m    376\u001b[0m     join\u001b[38;5;241m=\u001b[39mjoin,\n\u001b[0;32m    377\u001b[0m     keys\u001b[38;5;241m=\u001b[39mkeys,\n\u001b[0;32m    378\u001b[0m     levels\u001b[38;5;241m=\u001b[39mlevels,\n\u001b[0;32m    379\u001b[0m     names\u001b[38;5;241m=\u001b[39mnames,\n\u001b[0;32m    380\u001b[0m     verify_integrity\u001b[38;5;241m=\u001b[39mverify_integrity,\n\u001b[0;32m    381\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m    382\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m    383\u001b[0m )\n\u001b[0;32m    385\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:429\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    426\u001b[0m     objs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 429\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    431\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    432\u001b[0m     objs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs))\n",
      "\u001b[1;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "#select the event with comparable result\n",
    "csv_list = [pd.read_csv(fil) for fil in os.listdir() if fil.endswith('swimming.csv') or fil.endswith('weightlifting.csv') or fil.endswith('athletics.csv')\n",
    "             or fil.endswith('cycling-track.csv') or fil.endswith('rowing.csv') or fil.endswith('sailing.csv')\n",
    "             ]\n",
    "data_olympic=pd.concat(csv_list)\n",
    "data_olympic.rename(columns={'Unnamed: 0': 'rank'},inplace=True)\n",
    "#drop rows if result==nan\n",
    "data_olympic.dropna(subset='result',inplace=True)\n",
    "#divide olympic_game columns into two columns olympic_host and olympic_game_year\n",
    "data_olympic['olympic_host']=data_olympic['olympic_game'].str.split(' ').str[0]\n",
    "data_olympic['olympic_game_year']=data_olympic['olympic_game'].str.split(' ').str[1]\n",
    "data_olympic.drop(columns='olympic_game',inplace=True)\n",
    "data_olympic['olympic_host']=data_olympic['olympic_host'].str.replace('Los','Los Angeles').str.replace('Mexico','Mexico City') #Manual solution :(\n",
    "data_olympic['olympic_game_year']=data_olympic['olympic_game_year'].str.replace('Angeles','1984').str.replace('City','1968') #Manual solution :(\n",
    "#create and separete by gender using column event and drop rows of mix events\n",
    "data_olympic['gender'] = data_olympic['event'].apply(lambda x: re.findall(r'\\b(men|women)\\b', x, flags=re.IGNORECASE)[0].lower() if re.findall(r'\\b(men|women)\\b', x, flags=re.IGNORECASE) else 'mix')\n",
    "data_olympic=data_olympic[data_olympic['gender']!='mix']\n",
    "#cleaning event\n",
    "data_olympic['event'] = data_olympic['event'].apply(lambda x: re.sub(r'\\b(?:men|women)\\b', '', x, flags=re.IGNORECASE).lower().strip())\n",
    "data_olympic['event']=data_olympic['event'].str.replace(\"'s\",'',)\n",
    "#filter by rank\n",
    "data_olympic=data_olympic[data_olympic['rank']<=2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.pivot_table(data_olympic,index=['discipline','event'],columns='gender',values='result',aggfunc='count').sort_values(by='women',ascending=False).head(20) #this can be a function that give you this list or dict with x top events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating comparable tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first event\n",
    "relay400=data_olympic[data_olympic['event']=='4x100m relay']\n",
    "relay400['result']=relay400['result'].str.replace('w','')\n",
    "relay400['result']=relay400['result'].astype(float)\n",
    "relay400s=pd.pivot_table(relay400, index=['olympic_game_year'], columns='gender', values='result', aggfunc='mean')\n",
    "relay400s['gap']=relay400s['men']-relay400s['women']\n",
    "relay400s.round(2).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Second event\n",
    "free100=data_olympic[data_olympic['event']=='100m']\n",
    "free100['result']=free100['result'].str.replace('w','')\n",
    "free100['result']=free100['result'].astype(float)\n",
    "free100s=pd.pivot_table(free100, index=['olympic_game_year'], columns='gender', values='result', aggfunc='mean')\n",
    "free100s['gap']=free100s['men']-free100s['women']\n",
    "free100s.round(2).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#third event\n",
    "high_jump=data_olympic[data_olympic['event']=='high jump']\n",
    "high_jump['result']=high_jump['result'].astype(float)\n",
    "high_jump_sum=pd.pivot_table(high_jump, index=['olympic_game_year'], columns='gender', values='result', aggfunc='mean')\n",
    "high_jump_sum['gap']=high_jump_sum['men']-high_jump_sum['women']\n",
    "high_jump_sum.round(2).sort_index()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
